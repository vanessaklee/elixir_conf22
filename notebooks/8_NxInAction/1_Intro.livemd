# Nx in Action / Ask the Right Questions

## Goal: DIY ML in Elixir

> <span style="font-size: 14px; font-style: italic;">"It is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility"</span>
> 
> <span style="font-size: 14px;">Hans Moravec, 1988</span>

In this notebook we will go over the steps for Machine Learning and how we will perform them using Elixir's Nx ecosystem. We have covered this topics previously, but here we present them in the context of how we will put them to use to train, test, and use a ML model to make predictions in Elixir.

<!-- livebook:{"break_markdown":true} -->

### The Plan

1. Define the problem
2. Prepare the data
3. Evaluate the algorithms
4. Choose the model
5. Make the predictions

<!-- livebook:{"break_markdown":true} -->

### Define the Problem

What is the problem you want to solve? What is the question you want to answer?

The challenge of machine learning, and artificial intelligence in general, is to ask the right questions and understand what the answers you get mean.

That means taking the time to look at the big picture. It is easy to get excited by the little accomplishment, especially in deep learning: each layer making progress and predictions printing out one after the other. Remember to keep asking yourself if you are asking the best questions and asking those questions of the best dataset.

We are humans asking machines to answer questions for us, but as humans our sensory perception evolved over billions of years of evolution.

<!-- livebook:{"break_markdown":true} -->

### Prepare the Data

Find a dataset relevant to the problem you are trying to solve.

Take a good look at the data. Look for potential problems, patterns, classifications, correlations and comparisons. Turn “abnormal” data into “normalized” data. Normalizing includes extraction, loading, and data cleaning to identify aberrant, missing, or outliers.

<!-- livebook:{"break_markdown":true} -->

### Evaluate the Algorithm

Try out different algorithms to find the best one to use in training the model. The wrong model can lead to poor performance.

<!-- livebook:{"break_markdown":true} -->

### Train the Model

Train and test the model. Tune hyperparameters.

<!-- livebook:{"break_markdown":true} -->

In Axon, that means making decisions about:

1. Epochs - how many training iterations to take over the dataset
2. Learning rate - size of the optimization step at each iteration
3. Loss - measure of the distance between the prediction from the label
4. Metric (optional) - choose metric to measure model's performance

<!-- livebook:{"break_markdown":true} -->

##### Epochs

An epoch refers to one complete pass of the training data through the algorithm. It is a hyperparameter. The maximum epochs to run the Axon loop for. Must be a non-negative integer (default is 1).

<!-- livebook:{"break_markdown":true} -->

##### Learning Rate

Implementations of common gradient-based optimization algorithms. Customize the Optimizer by passing in a learning rate. The learning rate is the amount the weights are updated during training. It is a hyperparameter. Use hyperparameters to fine tune your model.

<!-- livebook:{"break_markdown":true} -->

##### Loss Function

The loss function measures how well a model fits the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

<!-- livebook:{"break_markdown":true} -->

##### Optimizer

Axon currently supports the following optimizers. Read about them [here](https://github.com/elixir-nx/axon/blob/main/lib/axon/optimizers.ex#L58).

* Adabelief
* Adagrad
* Adam
* Adamw
* Fromage
* Lamb
* Noisy SGD
* Radam
* RMSProp
* SGD
* Yogi

The Axon optimization API does not depend on Axon models, so you are able to use the API to optimize any differentiable objective function.

Customize the optimizer by passing in a learning rate. The learning rate is the amount the weights are updated during training. It is a hyperparameter.

<!-- livebook:{"break_markdown":true} -->

##### Loss Function

The loss function measures how well a model fit the data set.The bigger the difference between the prediction and the ground truth (target or class), the higher the loss function value.

<!-- livebook:{"break_markdown":true} -->

Different loss functions can have input shape requirements in Axon. See [here](https://github.com/elixir-nx/axon/blob/main/lib/axon/losses.ex) for more details.

* :binary_cross_entropy
  * Used in binary classification problems.
  * Expects targets to encode probabilities between 0 and 1.
  * You can optionally set `from_logits: true` to indicate values are binary_cross_entropy(y_true, y_pred, opts \\ []) do
* :categorical_cross_entropy
  * Used for multi-class classification problems.
  * Expects target to encode a probability distribution along the last axis.
  * You can specify `from_logits: true` to indicate `y_pred` is a logits tensor.
* :categorical_hinge
* :hinge
* :kl_divergence
* :log_cosh
* :margin_ranking
* :soft_margin
* :mean_absolute_error
* :mean_squared_error
* :poisson

<!-- livebook:{"break_markdown":true} -->

#### Metric

Metrics are used to measure the performance of models.

##### Accuracy

The number of times the model was correct overall

_How accurate is the model at making predictions on out of sample data?_

`(true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)`

##### Precision

Tells you how good the model is at predicting a specific category.

_Out of all positive predictions made by the model, what percentage are truly positive?_

The number of actual positive classes (`true_pos`) found in the dataset relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as positive (`false_pos`).

(true_pos) / (true_pos + false_pos)

##### Recall

Tells you how many times the model was able to detect a specific category.

_Of all of the actual positive classes in the dataset, how many of them did the model recall?_

The number of actual positive classes (`true_pos`) relative to the number of actual positive classes (`true_pos`) plus classes that were falsely identified as negative (`false_neg`) — those misidentified as negative.

`(true_pos) / (true_pos + false_neg)`

###### Confusion Matrix

|                          | Positive (Actual) | Negative (Actual) |
| ------------------------ | ----------------- | ----------------- |
| **Positive (Predicted)** | True Positive     | False Positive    |
| **Negative (Predicted)** | True Negative     | False Negative    |

<!-- livebook:{"break_markdown":true} -->

[<span style="float: left; color: #800080; font-weight: bold; font-family: FreeMono, monospace">< previous chapter: Nx Plus</span>](../7_NxPlus/1_Explorer.livemd)

[<span style="float: right; color: #800080; font-weight: bold; font-family: FreeMono, monospace">next ></span>](Iris/1_Data.livemd)
